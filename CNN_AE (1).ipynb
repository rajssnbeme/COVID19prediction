{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN-AE.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-oDlnRCigjQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D,Conv1D\n",
        "from keras.layers import MaxPooling1D,MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3moH4lRljwVR",
        "outputId": "3a28521d-764f-4476-9b74-96954458a2ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/gdrive/MyDrive/Risk Prediction/Dataset/AEdatasetfinal.csv',index_col=[0])"
      ],
      "metadata": {
        "id": "tt2xAR4NjjlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "id": "SPyHvnbDj51Z",
        "outputId": "c15774de-df6a-47bb-92d7-d7c3d5f1ba4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Derivation cohort  LOS_Y  LOS  Death  Severity  Black  White  Asian  \\\n",
              "5626                0.0    1.0  1.0    1.0       1.0    0.0    0.0    0.0   \n",
              "5627                0.0    1.0  1.0    1.0       1.0    0.0    0.0    0.0   \n",
              "5628                0.0    1.0  1.0    1.0       1.0    0.0    0.0    0.0   \n",
              "5629                0.0    1.0  1.0    1.0       1.0    0.0    0.0    0.0   \n",
              "5630                0.0    1.0  1.0    1.0       1.0    0.0    0.0    0.0   \n",
              "\n",
              "      Latino   MI  ...  Ferritin > 300  CrctProtYes  CrctProtein  \\\n",
              "5626     0.0  0.0  ...             0.0          1.0          1.0   \n",
              "5627     0.0  0.0  ...             0.0          1.0          1.0   \n",
              "5628     0.0  0.0  ...             0.0          1.0          1.0   \n",
              "5629     0.0  0.0  ...             0.0          1.0          1.0   \n",
              "5630     0.0  0.0  ...             0.0          1.0          1.0   \n",
              "\n",
              "      C-Reactive Prot > 10  ProCalCYes  Procalcitonin  Procalciton > 0.1  \\\n",
              "5626                   1.0         0.0            1.0                1.0   \n",
              "5627                   1.0         0.0            1.0                1.0   \n",
              "5628                   1.0         0.0            1.0                1.0   \n",
              "5629                   1.0         0.0            1.0                1.0   \n",
              "5630                   1.0         0.0            1.0                1.0   \n",
              "\n",
              "      TropYes  Troponin  Troponin > 0.1  \n",
              "5626      1.0       0.0             0.0  \n",
              "5627      1.0       0.0             0.0  \n",
              "5628      1.0       0.0             0.0  \n",
              "5629      1.0       0.0             0.0  \n",
              "5630      1.0       0.0             0.0  \n",
              "\n",
              "[5 rows x 84 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-205f589d-85c8-48a5-bfb7-8b50d685c5a6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Derivation cohort</th>\n",
              "      <th>LOS_Y</th>\n",
              "      <th>LOS</th>\n",
              "      <th>Death</th>\n",
              "      <th>Severity</th>\n",
              "      <th>Black</th>\n",
              "      <th>White</th>\n",
              "      <th>Asian</th>\n",
              "      <th>Latino</th>\n",
              "      <th>MI</th>\n",
              "      <th>...</th>\n",
              "      <th>Ferritin &gt; 300</th>\n",
              "      <th>CrctProtYes</th>\n",
              "      <th>CrctProtein</th>\n",
              "      <th>C-Reactive Prot &gt; 10</th>\n",
              "      <th>ProCalCYes</th>\n",
              "      <th>Procalcitonin</th>\n",
              "      <th>Procalciton &gt; 0.1</th>\n",
              "      <th>TropYes</th>\n",
              "      <th>Troponin</th>\n",
              "      <th>Troponin &gt; 0.1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5626</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5627</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5628</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5629</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5630</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 84 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-205f589d-85c8-48a5-bfb7-8b50d685c5a6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-205f589d-85c8-48a5-bfb7-8b50d685c5a6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-205f589d-85c8-48a5-bfb7-8b50d685c5a6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.drop(labels=5630,axis=0)\n",
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAJSHwjqSvF4",
        "outputId": "4d9896d5-e429-41aa-835f-1d21fd370e8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5630, 84)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#show only column names with more than 2 unique values, discarding all boolean ones. \n",
        "cols = ((df.dtypes != 'object') & (df.nunique() > 2))\n",
        "cols = cols.drop(['LOS', 'Severity', 'Age.1', 'AgeScore','CrtnScore','PltsScore']) #drop items that show categories, scores, etc.\n",
        "normFeatures = list(cols[cols].index)\n",
        "print(normFeatures)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9iQiUJdWE1F",
        "outputId": "3692971b-afcf-4bca-a433-4afd3130dac0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['PVD', 'OsSats', 'Temp', 'MAP', 'Ddimer', 'Plts', 'INR', 'BUN', 'Creatinine', 'Sodium', 'Glucose', 'AST', 'ALT', 'WBC', 'Lympho', 'IL6', 'Ferritin', 'CrctProtein', 'Procalcitonin', 'Troponin']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Apply StandardScaler() in a couple of columns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "normalizedData = df.copy()\n",
        "\n",
        "for name in normFeatures:\n",
        "    normalizedData[name] = (normalizedData[name] - normalizedData[name].mean()) / normalizedData[name].std()\n",
        "\n",
        "normalizedData[normFeatures]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "2YwAR4U0WOTY",
        "outputId": "f7abed86-475c-462f-f57f-ae565d2f32f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           PVD    OsSats      Temp       MAP    Ddimer      Plts       INR  \\\n",
              "0    -0.477965  0.536128  0.460340  0.913869 -0.456032  0.114666  0.013302   \n",
              "1     1.939773  0.374218  0.422209  0.467161  0.000066  0.479565  0.341768   \n",
              "2     1.939773  0.644068  0.618996 -0.033142 -0.016296 -0.391726 -0.096187   \n",
              "3    -0.477965  0.293263  0.596525  0.913869  0.032791  0.248710  0.232279   \n",
              "4    -0.477965  0.482158  0.540009  0.958352 -0.247413 -0.481089 -0.096187   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "5625 -0.477965 -2.000456 -1.983500 -1.810598 -0.380357 -1.404509 -0.096187   \n",
              "5626 -0.477965 -2.000456 -1.983500 -1.810598 -0.380357 -1.404509 -0.096187   \n",
              "5627 -0.477965 -2.000456 -1.983500 -1.810598 -0.380357 -1.404509 -0.096187   \n",
              "5628 -0.477965 -2.000456 -1.983500 -1.810598 -0.380357 -1.404509 -0.096187   \n",
              "5629 -0.477965 -2.000456 -1.983500 -1.810598 -0.380357 -1.404509 -0.096187   \n",
              "\n",
              "           BUN  Creatinine    Sodium   Glucose       AST       ALT       WBC  \\\n",
              "0    -0.229524   -0.448203  0.448275  0.037999 -0.143546 -0.218221 -0.227118   \n",
              "1     6.072065    3.038012  0.484074  1.226989  0.169507 -0.039807  0.680259   \n",
              "2     2.058948    0.683791  0.537772  0.107939 -0.111710 -0.178574 -0.158377   \n",
              "3    -0.760185    2.668883  0.394577 -0.832374 -0.048038 -0.188486 -0.488332   \n",
              "4    -0.760185    0.331068  0.698868 -0.832374 -0.079874 -0.079455 -0.213370   \n",
              "...        ...         ...       ...       ...       ...       ...       ...   \n",
              "5625 -0.727018   -0.325160 -1.950254 -0.824602 -0.276195 -0.347076 -0.859532   \n",
              "5626 -0.727018   -0.325160 -1.950254 -0.824602 -0.276195 -0.347076 -0.859532   \n",
              "5627 -0.727018   -0.325160 -1.950254 -0.824602 -0.276195 -0.347076 -0.859532   \n",
              "5628 -0.727018   -0.325160 -1.950254 -0.824602 -0.276195 -0.347076 -0.859532   \n",
              "5629 -0.727018   -0.325160 -1.950254 -0.824602 -0.276195 -0.347076 -0.859532   \n",
              "\n",
              "        Lympho       IL6  Ferritin  CrctProtein  Procalcitonin  Troponin  \n",
              "0     0.031845 -0.051162 -0.309308    -0.766004      -0.259913 -0.320352  \n",
              "1    -0.215355 -0.051162  0.000222     0.563854      -0.155251  2.999007  \n",
              "2    -0.170409  0.107076 -0.089315     2.318514      -0.050589 -0.348246  \n",
              "3    -0.170409 -0.051162 -0.023911     0.887081       1.013478 -0.208777  \n",
              "4     0.031845 -0.031925  0.282121     0.259097      -0.259913 -0.320352  \n",
              "...        ...       ...       ...          ...            ...       ...  \n",
              "5625 -0.058046 -0.050673 -0.308958    -0.719819      -0.085476 -0.348246  \n",
              "5626 -0.058046 -0.050673 -0.308958    -0.719819      -0.085476 -0.348246  \n",
              "5627 -0.058046 -0.050673 -0.308958    -0.719819      -0.085476 -0.348246  \n",
              "5628 -0.058046 -0.050673 -0.308958    -0.719819      -0.085476 -0.348246  \n",
              "5629 -0.058046 -0.050673 -0.308958    -0.719819      -0.085476 -0.348246  \n",
              "\n",
              "[5630 rows x 20 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c521630e-2803-44b9-9cea-54295e40f56f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PVD</th>\n",
              "      <th>OsSats</th>\n",
              "      <th>Temp</th>\n",
              "      <th>MAP</th>\n",
              "      <th>Ddimer</th>\n",
              "      <th>Plts</th>\n",
              "      <th>INR</th>\n",
              "      <th>BUN</th>\n",
              "      <th>Creatinine</th>\n",
              "      <th>Sodium</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>AST</th>\n",
              "      <th>ALT</th>\n",
              "      <th>WBC</th>\n",
              "      <th>Lympho</th>\n",
              "      <th>IL6</th>\n",
              "      <th>Ferritin</th>\n",
              "      <th>CrctProtein</th>\n",
              "      <th>Procalcitonin</th>\n",
              "      <th>Troponin</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.477965</td>\n",
              "      <td>0.536128</td>\n",
              "      <td>0.460340</td>\n",
              "      <td>0.913869</td>\n",
              "      <td>-0.456032</td>\n",
              "      <td>0.114666</td>\n",
              "      <td>0.013302</td>\n",
              "      <td>-0.229524</td>\n",
              "      <td>-0.448203</td>\n",
              "      <td>0.448275</td>\n",
              "      <td>0.037999</td>\n",
              "      <td>-0.143546</td>\n",
              "      <td>-0.218221</td>\n",
              "      <td>-0.227118</td>\n",
              "      <td>0.031845</td>\n",
              "      <td>-0.051162</td>\n",
              "      <td>-0.309308</td>\n",
              "      <td>-0.766004</td>\n",
              "      <td>-0.259913</td>\n",
              "      <td>-0.320352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.939773</td>\n",
              "      <td>0.374218</td>\n",
              "      <td>0.422209</td>\n",
              "      <td>0.467161</td>\n",
              "      <td>0.000066</td>\n",
              "      <td>0.479565</td>\n",
              "      <td>0.341768</td>\n",
              "      <td>6.072065</td>\n",
              "      <td>3.038012</td>\n",
              "      <td>0.484074</td>\n",
              "      <td>1.226989</td>\n",
              "      <td>0.169507</td>\n",
              "      <td>-0.039807</td>\n",
              "      <td>0.680259</td>\n",
              "      <td>-0.215355</td>\n",
              "      <td>-0.051162</td>\n",
              "      <td>0.000222</td>\n",
              "      <td>0.563854</td>\n",
              "      <td>-0.155251</td>\n",
              "      <td>2.999007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.939773</td>\n",
              "      <td>0.644068</td>\n",
              "      <td>0.618996</td>\n",
              "      <td>-0.033142</td>\n",
              "      <td>-0.016296</td>\n",
              "      <td>-0.391726</td>\n",
              "      <td>-0.096187</td>\n",
              "      <td>2.058948</td>\n",
              "      <td>0.683791</td>\n",
              "      <td>0.537772</td>\n",
              "      <td>0.107939</td>\n",
              "      <td>-0.111710</td>\n",
              "      <td>-0.178574</td>\n",
              "      <td>-0.158377</td>\n",
              "      <td>-0.170409</td>\n",
              "      <td>0.107076</td>\n",
              "      <td>-0.089315</td>\n",
              "      <td>2.318514</td>\n",
              "      <td>-0.050589</td>\n",
              "      <td>-0.348246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.477965</td>\n",
              "      <td>0.293263</td>\n",
              "      <td>0.596525</td>\n",
              "      <td>0.913869</td>\n",
              "      <td>0.032791</td>\n",
              "      <td>0.248710</td>\n",
              "      <td>0.232279</td>\n",
              "      <td>-0.760185</td>\n",
              "      <td>2.668883</td>\n",
              "      <td>0.394577</td>\n",
              "      <td>-0.832374</td>\n",
              "      <td>-0.048038</td>\n",
              "      <td>-0.188486</td>\n",
              "      <td>-0.488332</td>\n",
              "      <td>-0.170409</td>\n",
              "      <td>-0.051162</td>\n",
              "      <td>-0.023911</td>\n",
              "      <td>0.887081</td>\n",
              "      <td>1.013478</td>\n",
              "      <td>-0.208777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.477965</td>\n",
              "      <td>0.482158</td>\n",
              "      <td>0.540009</td>\n",
              "      <td>0.958352</td>\n",
              "      <td>-0.247413</td>\n",
              "      <td>-0.481089</td>\n",
              "      <td>-0.096187</td>\n",
              "      <td>-0.760185</td>\n",
              "      <td>0.331068</td>\n",
              "      <td>0.698868</td>\n",
              "      <td>-0.832374</td>\n",
              "      <td>-0.079874</td>\n",
              "      <td>-0.079455</td>\n",
              "      <td>-0.213370</td>\n",
              "      <td>0.031845</td>\n",
              "      <td>-0.031925</td>\n",
              "      <td>0.282121</td>\n",
              "      <td>0.259097</td>\n",
              "      <td>-0.259913</td>\n",
              "      <td>-0.320352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5625</th>\n",
              "      <td>-0.477965</td>\n",
              "      <td>-2.000456</td>\n",
              "      <td>-1.983500</td>\n",
              "      <td>-1.810598</td>\n",
              "      <td>-0.380357</td>\n",
              "      <td>-1.404509</td>\n",
              "      <td>-0.096187</td>\n",
              "      <td>-0.727018</td>\n",
              "      <td>-0.325160</td>\n",
              "      <td>-1.950254</td>\n",
              "      <td>-0.824602</td>\n",
              "      <td>-0.276195</td>\n",
              "      <td>-0.347076</td>\n",
              "      <td>-0.859532</td>\n",
              "      <td>-0.058046</td>\n",
              "      <td>-0.050673</td>\n",
              "      <td>-0.308958</td>\n",
              "      <td>-0.719819</td>\n",
              "      <td>-0.085476</td>\n",
              "      <td>-0.348246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5626</th>\n",
              "      <td>-0.477965</td>\n",
              "      <td>-2.000456</td>\n",
              "      <td>-1.983500</td>\n",
              "      <td>-1.810598</td>\n",
              "      <td>-0.380357</td>\n",
              "      <td>-1.404509</td>\n",
              "      <td>-0.096187</td>\n",
              "      <td>-0.727018</td>\n",
              "      <td>-0.325160</td>\n",
              "      <td>-1.950254</td>\n",
              "      <td>-0.824602</td>\n",
              "      <td>-0.276195</td>\n",
              "      <td>-0.347076</td>\n",
              "      <td>-0.859532</td>\n",
              "      <td>-0.058046</td>\n",
              "      <td>-0.050673</td>\n",
              "      <td>-0.308958</td>\n",
              "      <td>-0.719819</td>\n",
              "      <td>-0.085476</td>\n",
              "      <td>-0.348246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5627</th>\n",
              "      <td>-0.477965</td>\n",
              "      <td>-2.000456</td>\n",
              "      <td>-1.983500</td>\n",
              "      <td>-1.810598</td>\n",
              "      <td>-0.380357</td>\n",
              "      <td>-1.404509</td>\n",
              "      <td>-0.096187</td>\n",
              "      <td>-0.727018</td>\n",
              "      <td>-0.325160</td>\n",
              "      <td>-1.950254</td>\n",
              "      <td>-0.824602</td>\n",
              "      <td>-0.276195</td>\n",
              "      <td>-0.347076</td>\n",
              "      <td>-0.859532</td>\n",
              "      <td>-0.058046</td>\n",
              "      <td>-0.050673</td>\n",
              "      <td>-0.308958</td>\n",
              "      <td>-0.719819</td>\n",
              "      <td>-0.085476</td>\n",
              "      <td>-0.348246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5628</th>\n",
              "      <td>-0.477965</td>\n",
              "      <td>-2.000456</td>\n",
              "      <td>-1.983500</td>\n",
              "      <td>-1.810598</td>\n",
              "      <td>-0.380357</td>\n",
              "      <td>-1.404509</td>\n",
              "      <td>-0.096187</td>\n",
              "      <td>-0.727018</td>\n",
              "      <td>-0.325160</td>\n",
              "      <td>-1.950254</td>\n",
              "      <td>-0.824602</td>\n",
              "      <td>-0.276195</td>\n",
              "      <td>-0.347076</td>\n",
              "      <td>-0.859532</td>\n",
              "      <td>-0.058046</td>\n",
              "      <td>-0.050673</td>\n",
              "      <td>-0.308958</td>\n",
              "      <td>-0.719819</td>\n",
              "      <td>-0.085476</td>\n",
              "      <td>-0.348246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5629</th>\n",
              "      <td>-0.477965</td>\n",
              "      <td>-2.000456</td>\n",
              "      <td>-1.983500</td>\n",
              "      <td>-1.810598</td>\n",
              "      <td>-0.380357</td>\n",
              "      <td>-1.404509</td>\n",
              "      <td>-0.096187</td>\n",
              "      <td>-0.727018</td>\n",
              "      <td>-0.325160</td>\n",
              "      <td>-1.950254</td>\n",
              "      <td>-0.824602</td>\n",
              "      <td>-0.276195</td>\n",
              "      <td>-0.347076</td>\n",
              "      <td>-0.859532</td>\n",
              "      <td>-0.058046</td>\n",
              "      <td>-0.050673</td>\n",
              "      <td>-0.308958</td>\n",
              "      <td>-0.719819</td>\n",
              "      <td>-0.085476</td>\n",
              "      <td>-0.348246</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5630 rows × 20 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c521630e-2803-44b9-9cea-54295e40f56f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c521630e-2803-44b9-9cea-54295e40f56f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c521630e-2803-44b9-9cea-54295e40f56f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = normalizedData"
      ],
      "metadata": {
        "id": "1D9V7a7fWhh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "req_features=['LOS_Y', 'LOS', 'Severity',\n",
        "       'All CNS', 'Pure CNS', 'Age.1', 'AgeScore', 'O2 Sat < 94', 'MAP < 70', 'Ddimer', 'D-Dimer > 3', 'PltsScore', 'INRYes', 'INR', 'INR > 1.2', 'BUN',\n",
        "       'BUN > 30', 'Creatinine', 'CrtnScore',\n",
        "       'Sodium < 139 or > 154', 'AST', 'AST > 40', 'WBC', 'WBC <1.8 or > 4.8',\n",
        "       'Lymphocytes < 1', 'IL6 > 150',\n",
        "       'Ferritin', 'Ferritin > 300', 'CrctProtein',\n",
        "       'C-Reactive Prot > 10', 'Procalcitonin',\n",
        "       'Procalciton > 0.1', 'TropYes', 'Troponin', 'Troponin > 0.1']"
      ],
      "metadata": {
        "id": "CF8S0dCqi7vy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=df[req_features]"
      ],
      "metadata": {
        "id": "oc3odoy1jD3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59_tLC-LO02J",
        "outputId": "5ecfc11d-a2dc-4be9-d4a5-0c5a4d51860d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5630, 35)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y=df['Death']"
      ],
      "metadata": {
        "id": "gJacRiiIl4qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmX6rKx8l6Aa",
        "outputId": "20e1ba57-bc5c-4f4d-8fbb-d1bd4c76f2ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5630, 35)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv1D(filters=256, kernel_size=3, strides=1, activation='relu', input_shape=(35,1)))\n",
        "model.add(MaxPooling1D(pool_size=2 ,strides=1))\n",
        "#model.add(Conv1D(filters=256, kernel_size=3, strides=1, activation='relu'))\n",
        "#model.add(Conv1D(filters=512, kernel_size=3, strides=1, activation='relu'))\n",
        "model.add(Conv1D(filters=256, kernel_size=3, strides=1, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2 ,strides=1))\n",
        "model.add(Conv1D(filters=256, kernel_size=3, strides=1, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2 ,strides=1))\n",
        "#model.add(Conv1D(filters=256, kernel_size=3, strides=1, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2 ,strides=1))\n",
        "#model.add(Reshape((64,32,3), input_shape=(24,256)))\n",
        "#model.add(Reshape((11,340,3), input_shape=(11,4,256)))\n",
        "#model.add(VGG19(include_top=False,weights=\"imagenet\",input_shape=(64,32,3)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etjyb0Zkl_u6",
        "outputId": "0d256f50-16fd-479f-8ee0-2f686d0b22d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d (Conv1D)             (None, 33, 256)           1024      \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 32, 256)          0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 30, 256)           196864    \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPooling  (None, 29, 256)          0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 27, 256)           196864    \n",
            "                                                                 \n",
            " max_pooling1d_2 (MaxPooling  (None, 26, 256)          0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " max_pooling1d_3 (MaxPooling  (None, 25, 256)          0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 6400)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                409664    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 806,529\n",
            "Trainable params: 806,529\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.optimizers import SGD, Adam, Adamax, Adadelta\n",
        "import sklearn\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "metadata": {
        "id": "XLnEQodFmC7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt=Adam(learning_rate= 0.0001)  \n",
        "METRICS = [\n",
        "      keras.metrics.TruePositives(name='tp'),\n",
        "      keras.metrics.FalsePositives(name='fp'),\n",
        "      keras.metrics.TrueNegatives(name='tn'),\n",
        "      keras.metrics.FalseNegatives(name='fn'), \n",
        "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "      keras.metrics.Precision(name='Precision'),\n",
        "      keras.metrics.Recall(name='Recall'),\n",
        "      keras.metrics.AUC(name='Auc'),\n",
        "      \n",
        "]"
      ],
      "metadata": {
        "id": "tsWb1eO6mEjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer= opt,\n",
        "              metrics='accuracy')"
      ],
      "metadata": {
        "id": "7g2m4ocXmHFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import classification_report\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score, f1_score, recall_score, roc_auc_score, confusion_matrix"
      ],
      "metadata": {
        "id": "kG_9s7AzmKe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kfold = KFold(n_splits=10, shuffle=False)"
      ],
      "metadata": {
        "id": "BvGetVS_mNJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "j=1           \n",
        "i=1\n",
        "for train_index, test_index in kfold.split(df):\n",
        "    X4 = df.iloc[train_index].loc[:, req_features]\n",
        "    X5 = df.iloc[test_index][req_features]\n",
        "    y4= df.iloc[train_index].loc[:,'Death']\n",
        "    y5 = df.loc[test_index]['Death']\n",
        "\n",
        "    X6= X4.to_numpy('float64')\n",
        "    X7=np.resize(X6,(5067,35,1))\n",
        "    X8= X5.to_numpy('float64')\n",
        "    X9=np.resize(X8,(563,35,1))\n",
        "    y6=y4.to_numpy('int')    #train output\n",
        "    y7=y5.to_numpy('int')\n",
        "    train_x = X7\n",
        "    train_y = y6\n",
        "    validation_x = X9\n",
        "    validation_y = y7\n",
        "    history= model.fit(train_x , train_y, epochs=10, batch_size=12,verbose=1) #Training the model\n",
        "    ypre=model.predict(validation_x)\n",
        "    pre_y=np.round(abs(ypre))\n",
        "    print(f\"Accuracy for the fold no. {i} on the test set: {accuracy_score(validation_y, pre_y )}\")\n",
        "    accuracy = accuracy_score(validation_y, pre_y)\n",
        "    print('Accuracy: %f' % accuracy)\n",
        "    matrix = confusion_matrix(validation_y, pre_y)\n",
        "    print(matrix)\n",
        "    i+= 1 \n",
        "    j=j+1 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cikPOrImQyr",
        "outputId": "1437a267-083c-43f1-d2fd-afa58cf7aaaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "423/423 [==============================] - 15s 8ms/step - loss: 0.4756 - accuracy: 0.7916\n",
            "Epoch 2/10\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.4062 - accuracy: 0.8097\n",
            "Epoch 3/10\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.3881 - accuracy: 0.8139\n",
            "Epoch 4/10\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.3746 - accuracy: 0.8240\n",
            "Epoch 5/10\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.3572 - accuracy: 0.8346\n",
            "Epoch 6/10\n",
            "423/423 [==============================] - 4s 8ms/step - loss: 0.3502 - accuracy: 0.8403\n",
            "Epoch 7/10\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.3457 - accuracy: 0.8427\n",
            "Epoch 8/10\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.3363 - accuracy: 0.8470\n",
            "Epoch 9/10\n",
            "423/423 [==============================] - 4s 8ms/step - loss: 0.3361 - accuracy: 0.8494\n",
            "Epoch 10/10\n",
            "423/423 [==============================] - 4s 8ms/step - loss: 0.3301 - accuracy: 0.8532\n",
            "Accuracy for the fold no. 1 on the test set: 0.8063943161634103\n",
            "Accuracy: 0.806394\n",
            "[[391  19]\n",
            " [ 90  63]]\n",
            "Epoch 1/10\n",
            "423/423 [==============================] - 4s 9ms/step - loss: 0.3309 - accuracy: 0.8553\n",
            "Epoch 2/10\n",
            "423/423 [==============================] - 4s 9ms/step - loss: 0.3306 - accuracy: 0.8500\n",
            "Epoch 3/10\n",
            "423/423 [==============================] - 4s 8ms/step - loss: 0.3260 - accuracy: 0.8524\n",
            "Epoch 4/10\n",
            "423/423 [==============================] - 4s 8ms/step - loss: 0.3255 - accuracy: 0.8557\n",
            "Epoch 5/10\n",
            "423/423 [==============================] - 4s 8ms/step - loss: 0.3191 - accuracy: 0.8583\n",
            "Epoch 6/10\n",
            "423/423 [==============================] - 4s 9ms/step - loss: 0.3195 - accuracy: 0.8615\n",
            "Epoch 7/10\n",
            "423/423 [==============================] - 4s 9ms/step - loss: 0.3147 - accuracy: 0.8615\n",
            "Epoch 8/10\n",
            "423/423 [==============================] - 4s 9ms/step - loss: 0.3217 - accuracy: 0.8581\n",
            "Epoch 9/10\n",
            "423/423 [==============================] - 4s 8ms/step - loss: 0.3114 - accuracy: 0.8615\n",
            "Epoch 10/10\n",
            "423/423 [==============================] - 4s 8ms/step - loss: 0.3122 - accuracy: 0.8622\n",
            "Accuracy for the fold no. 2 on the test set: 0.8241563055062167\n",
            "Accuracy: 0.824156\n",
            "[[396  19]\n",
            " [ 80  68]]\n",
            "Epoch 1/10\n",
            "423/423 [==============================] - 4s 8ms/step - loss: 0.3101 - accuracy: 0.8615\n",
            "Epoch 2/10\n",
            "423/423 [==============================] - 4s 9ms/step - loss: 0.3035 - accuracy: 0.8692\n",
            "Epoch 3/10\n",
            "423/423 [==============================] - 4s 8ms/step - loss: 0.3048 - accuracy: 0.8666\n",
            "Epoch 4/10\n",
            "423/423 [==============================] - 4s 9ms/step - loss: 0.2979 - accuracy: 0.8695\n",
            "Epoch 5/10\n",
            "423/423 [==============================] - 4s 9ms/step - loss: 0.3006 - accuracy: 0.8656\n",
            "Epoch 6/10\n",
            "423/423 [==============================] - 4s 8ms/step - loss: 0.2916 - accuracy: 0.8717\n",
            "Epoch 7/10\n",
            "423/423 [==============================] - 4s 8ms/step - loss: 0.2920 - accuracy: 0.8709\n",
            "Epoch 8/10\n",
            "423/423 [==============================] - 4s 8ms/step - loss: 0.2893 - accuracy: 0.8739\n",
            "Epoch 9/10\n",
            "423/423 [==============================] - 4s 8ms/step - loss: 0.2851 - accuracy: 0.8735\n",
            "Epoch 10/10\n",
            "423/423 [==============================] - 4s 9ms/step - loss: 0.2785 - accuracy: 0.8770\n",
            "Accuracy for the fold no. 3 on the test set: 0.8383658969804618\n",
            "Accuracy: 0.838366\n",
            "[[396   7]\n",
            " [ 84  76]]\n",
            "Epoch 1/10\n",
            "423/423 [==============================] - 4s 9ms/step - loss: 0.2884 - accuracy: 0.8759\n",
            "Epoch 2/10\n",
            "423/423 [==============================] - 4s 8ms/step - loss: 0.2886 - accuracy: 0.8719\n",
            "Epoch 3/10\n",
            "423/423 [==============================] - 4s 8ms/step - loss: 0.2809 - accuracy: 0.8772\n",
            "Epoch 4/10\n",
            "423/423 [==============================] - 4s 9ms/step - loss: 0.2771 - accuracy: 0.8761\n",
            "Epoch 5/10\n",
            "423/423 [==============================] - 4s 9ms/step - loss: 0.2723 - accuracy: 0.8808\n",
            "Epoch 6/10\n",
            "423/423 [==============================] - 4s 8ms/step - loss: 0.2677 - accuracy: 0.8804\n",
            "Epoch 7/10\n",
            "423/423 [==============================] - 4s 8ms/step - loss: 0.2644 - accuracy: 0.8859\n",
            "Epoch 8/10\n",
            "423/423 [==============================] - 4s 8ms/step - loss: 0.2595 - accuracy: 0.8859\n",
            "Epoch 9/10\n",
            "423/423 [==============================] - 4s 9ms/step - loss: 0.2570 - accuracy: 0.8883\n",
            "Epoch 10/10\n",
            "423/423 [==============================] - 4s 9ms/step - loss: 0.2508 - accuracy: 0.8954\n",
            "Accuracy for the fold no. 4 on the test set: 0.8632326820603907\n",
            "Accuracy: 0.863233\n",
            "[[408  18]\n",
            " [ 59  78]]\n",
            "Epoch 1/10\n",
            "423/423 [==============================] - 4s 9ms/step - loss: 0.2528 - accuracy: 0.8907\n",
            "Epoch 2/10\n",
            "423/423 [==============================] - 4s 9ms/step - loss: 0.2508 - accuracy: 0.8938\n",
            "Epoch 3/10\n",
            "423/423 [==============================] - 4s 9ms/step - loss: 0.2408 - accuracy: 0.8956\n",
            "Epoch 4/10\n",
            "423/423 [==============================] - 4s 9ms/step - loss: 0.2385 - accuracy: 0.8958\n",
            "Epoch 5/10\n",
            "423/423 [==============================] - 4s 8ms/step - loss: 0.2323 - accuracy: 0.9009\n",
            "Epoch 6/10\n",
            "423/423 [==============================] - 4s 8ms/step - loss: 0.2263 - accuracy: 0.9019\n",
            "Epoch 7/10\n",
            "423/423 [==============================] - 4s 9ms/step - loss: 0.2288 - accuracy: 0.9007\n",
            "Epoch 8/10\n",
            "423/423 [==============================] - 4s 9ms/step - loss: 0.2238 - accuracy: 0.9063\n",
            "Epoch 9/10\n",
            "423/423 [==============================] - 4s 8ms/step - loss: 0.2134 - accuracy: 0.9070\n",
            "Epoch 10/10\n",
            "423/423 [==============================] - 4s 9ms/step - loss: 0.2098 - accuracy: 0.9108\n",
            "Accuracy for the fold no. 5 on the test set: 0.8561278863232682\n",
            "Accuracy: 0.856128\n",
            "[[404  20]\n",
            " [ 61  78]]\n",
            "Epoch 1/10\n",
            "423/423 [==============================] - 4s 9ms/step - loss: 0.2193 - accuracy: 0.9076\n",
            "Epoch 2/10\n",
            "423/423 [==============================] - 4s 8ms/step - loss: 0.2155 - accuracy: 0.9074\n",
            "Epoch 3/10\n",
            "423/423 [==============================] - 4s 9ms/step - loss: 0.2052 - accuracy: 0.9102\n",
            "Epoch 4/10\n",
            "423/423 [==============================] - 4s 8ms/step - loss: 0.2013 - accuracy: 0.9151\n",
            "Epoch 5/10\n",
            "423/423 [==============================] - 4s 8ms/step - loss: 0.1959 - accuracy: 0.9173\n",
            "Epoch 6/10\n",
            "423/423 [==============================] - 4s 9ms/step - loss: 0.1951 - accuracy: 0.9177\n",
            "Epoch 7/10\n",
            "423/423 [==============================] - 4s 8ms/step - loss: 0.1875 - accuracy: 0.9157\n",
            "Epoch 8/10\n",
            "423/423 [==============================] - 4s 8ms/step - loss: 0.1822 - accuracy: 0.9216\n",
            "Epoch 9/10\n",
            "423/423 [==============================] - 4s 8ms/step - loss: 0.1814 - accuracy: 0.9230\n",
            "Epoch 10/10\n",
            "423/423 [==============================] - 4s 8ms/step - loss: 0.1732 - accuracy: 0.9284\n",
            "Accuracy for the fold no. 6 on the test set: 0.8987566607460036\n",
            "Accuracy: 0.898757\n",
            "[[397  29]\n",
            " [ 28 109]]\n",
            "Epoch 1/10\n",
            "423/423 [==============================] - 4s 8ms/step - loss: 0.1798 - accuracy: 0.9266\n",
            "Epoch 2/10\n",
            "423/423 [==============================] - 4s 8ms/step - loss: 0.1667 - accuracy: 0.9333\n",
            "Epoch 3/10\n",
            "423/423 [==============================] - 4s 8ms/step - loss: 0.1602 - accuracy: 0.9349\n",
            "Epoch 4/10\n",
            "423/423 [==============================] - 4s 8ms/step - loss: 0.1557 - accuracy: 0.9337\n",
            "Epoch 5/10\n",
            "423/423 [==============================] - 4s 8ms/step - loss: 0.1526 - accuracy: 0.9404\n",
            "Epoch 6/10\n",
            "423/423 [==============================] - 4s 8ms/step - loss: 0.1471 - accuracy: 0.9416\n",
            "Epoch 7/10\n",
            "423/423 [==============================] - 4s 8ms/step - loss: 0.1412 - accuracy: 0.9449\n",
            "Epoch 8/10\n",
            "423/423 [==============================] - 4s 8ms/step - loss: 0.1377 - accuracy: 0.9440\n",
            "Epoch 9/10\n",
            "423/423 [==============================] - 4s 8ms/step - loss: 0.1305 - accuracy: 0.9491\n",
            "Epoch 10/10\n",
            "423/423 [==============================] - 4s 8ms/step - loss: 0.1283 - accuracy: 0.9511\n",
            "Accuracy for the fold no. 7 on the test set: 0.8703374777975134\n",
            "Accuracy: 0.870337\n",
            "[[394  43]\n",
            " [ 30  96]]\n",
            "Epoch 1/10\n",
            "423/423 [==============================] - 4s 8ms/step - loss: 0.1371 - accuracy: 0.9445\n",
            "Epoch 2/10\n",
            "423/423 [==============================] - 4s 9ms/step - loss: 0.1341 - accuracy: 0.9536\n",
            "Epoch 3/10\n",
            "423/423 [==============================] - 4s 9ms/step - loss: 0.1197 - accuracy: 0.9580\n",
            "Epoch 4/10\n",
            "423/423 [==============================] - 4s 9ms/step - loss: 0.1147 - accuracy: 0.9560\n",
            "Epoch 5/10\n",
            "423/423 [==============================] - 4s 9ms/step - loss: 0.1087 - accuracy: 0.9595\n",
            "Epoch 6/10\n",
            "423/423 [==============================] - 4s 8ms/step - loss: 0.1012 - accuracy: 0.9595\n",
            "Epoch 7/10\n",
            "423/423 [==============================] - 4s 9ms/step - loss: 0.1014 - accuracy: 0.9601\n",
            "Epoch 8/10\n",
            "423/423 [==============================] - 4s 9ms/step - loss: 0.0979 - accuracy: 0.9629\n",
            "Epoch 9/10\n",
            "423/423 [==============================] - 4s 9ms/step - loss: 0.1012 - accuracy: 0.9627\n",
            "Epoch 10/10\n",
            "423/423 [==============================] - 4s 9ms/step - loss: 0.0911 - accuracy: 0.9670\n",
            "Accuracy for the fold no. 8 on the test set: 0.8650088809946714\n",
            "Accuracy: 0.865009\n",
            "[[422  27]\n",
            " [ 49  65]]\n",
            "Epoch 1/10\n",
            "423/423 [==============================] - 4s 9ms/step - loss: 0.1185 - accuracy: 0.9546\n",
            "Epoch 2/10\n",
            "423/423 [==============================] - 4s 9ms/step - loss: 0.1087 - accuracy: 0.9591\n",
            "Epoch 3/10\n",
            "423/423 [==============================] - 4s 9ms/step - loss: 0.0984 - accuracy: 0.9663\n",
            "Epoch 4/10\n",
            "423/423 [==============================] - 4s 9ms/step - loss: 0.0906 - accuracy: 0.9668\n",
            "Epoch 5/10\n",
            "423/423 [==============================] - 4s 9ms/step - loss: 0.0935 - accuracy: 0.9659\n",
            "Epoch 6/10\n",
            "423/423 [==============================] - 4s 9ms/step - loss: 0.0886 - accuracy: 0.9664\n",
            "Epoch 7/10\n",
            "423/423 [==============================] - 4s 9ms/step - loss: 0.0871 - accuracy: 0.9670\n",
            "Epoch 8/10\n",
            "423/423 [==============================] - 4s 9ms/step - loss: 0.0838 - accuracy: 0.9718\n",
            "Epoch 9/10\n",
            "423/423 [==============================] - 4s 9ms/step - loss: 0.0828 - accuracy: 0.9680\n",
            "Epoch 10/10\n",
            "423/423 [==============================] - 4s 9ms/step - loss: 0.0759 - accuracy: 0.9720\n",
            "Accuracy for the fold no. 9 on the test set: 0.9396092362344582\n",
            "Accuracy: 0.939609\n",
            "[[158  15]\n",
            " [ 19 371]]\n",
            "Epoch 1/10\n",
            "423/423 [==============================] - 4s 9ms/step - loss: 0.0897 - accuracy: 0.9657\n",
            "Epoch 2/10\n",
            "423/423 [==============================] - 4s 9ms/step - loss: 0.0840 - accuracy: 0.9688\n",
            "Epoch 3/10\n",
            "423/423 [==============================] - 4s 9ms/step - loss: 0.0806 - accuracy: 0.9694\n",
            "Epoch 4/10\n",
            "423/423 [==============================] - 4s 9ms/step - loss: 0.0797 - accuracy: 0.9726\n",
            "Epoch 5/10\n",
            "423/423 [==============================] - 4s 9ms/step - loss: 0.0797 - accuracy: 0.9712\n",
            "Epoch 6/10\n",
            "423/423 [==============================] - 4s 9ms/step - loss: 0.0787 - accuracy: 0.9714\n",
            "Epoch 7/10\n",
            "423/423 [==============================] - 4s 8ms/step - loss: 0.0676 - accuracy: 0.9751\n",
            "Epoch 8/10\n",
            "423/423 [==============================] - 4s 9ms/step - loss: 0.0821 - accuracy: 0.9690\n",
            "Epoch 9/10\n",
            "423/423 [==============================] - 4s 9ms/step - loss: 0.0583 - accuracy: 0.9818\n",
            "Epoch 10/10\n",
            "423/423 [==============================] - 4s 9ms/step - loss: 0.0618 - accuracy: 0.9785\n",
            "Accuracy for the fold no. 10 on the test set: 1.0\n",
            "Accuracy: 1.000000\n",
            "[[563]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, Y, test_size=0.2, random_state=2)\n",
        "print(X_train1.shape)\n",
        "print(y_train1.shape)\n",
        "print(X_test1.shape)\n",
        "print(y_test1.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tv3bpovPjZ85",
        "outputId": "018fb6d6-9ad4-4507-9c50-b890a48a0a4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4504, 35)\n",
            "(4504,)\n",
            "(1126, 35)\n",
            "(1126,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test2=np.resize(X_test1,(1126,35,1))   #validation data  \n",
        "y_test2=y_test1.to_numpy('int')    #train output\n",
        "validation_x = X_test2\n",
        "validation_y = y_test2\n",
        "ypre=model.predict(validation_x)\n",
        "pre_y=np.where(ypre>0.5, 1, 0)\n",
        "accuracy = accuracy_score(validation_y, pre_y)\n",
        "auc=metrics.roc_auc_score(validation_y, pre_y)\n",
        "print('Accuracy: %f' % accuracy)\n",
        "print('AUC: %f' % auc)\n",
        "print('Classification Report')\n",
        "target_names = ['Recovered', 'Died']\n",
        "print(classification_report(validation_y, pre_y, target_names=target_names))\n",
        "# confusion matrix\n",
        "matrix = confusion_matrix(validation_y, pre_y)\n",
        "print(matrix)\n",
        "ax=plt.subplot()\n",
        "sns.heatmap(matrix,annot=True,ax=ax,cmap='OrRd_r', fmt='g')#annot=True to annotate cells, fmt='g' numbers not scientific form\n",
        "ax.set_xlabel('Predicted labels'); \n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix'); \n",
        "ax.xaxis.set_ticklabels(['Died', 'recovered']); \n",
        "ax.yaxis.set_ticklabels(['Died', 'recovered']);\n",
        "ax.set(yticks=[0, 2], \n",
        "       xticks=[0.5, 1.5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623
        },
        "id": "OiMFcBhpj6yH",
        "outputId": "64c39e70-93d9-449d-c5f9-310ebdaa7805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.981350\n",
            "AUC: 0.978106\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Recovered       0.98      0.99      0.99       730\n",
            "        Died       0.98      0.97      0.97       396\n",
            "\n",
            "    accuracy                           0.98      1126\n",
            "   macro avg       0.98      0.98      0.98      1126\n",
            "weighted avg       0.98      0.98      0.98      1126\n",
            "\n",
            "[[722   8]\n",
            " [ 13 383]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[<matplotlib.axis.YTick at 0x7fdb365e3750>,\n",
              "  <matplotlib.axis.YTick at 0x7fdb366dbf50>],\n",
              " [<matplotlib.axis.XTick at 0x7fdb3678a150>,\n",
              "  <matplotlib.axis.XTick at 0x7fdb366db690>]]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "precision, recall, thresholds = precision_recall_curve(validation_y,pre_y)\n",
        "plt.plot(precision,recall)\n",
        "plt.ylabel('Recall')\n",
        "plt.xlabel('Precision')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "XE2S2HsuxBYl",
        "outputId": "576009c0-084f-4372-eff5-931d04e49f92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}